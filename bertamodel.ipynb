{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a3bd32-f4e7-42d9-8179-f138e4aef9fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.1.post1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.12.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.4.1.post1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached scipy-1.12.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "Using cached threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 scipy-1.12.0 threadpoolctl-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05d4b17-4d4e-46b3-a9b7-a5e9286e635d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-intel==2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached h5py-3.10.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.8.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached wrapt-1.14.1-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached grpcio-1.62.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached tensorflow-2.15.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl (300.9 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Using cached h5py-3.10.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.14.1-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Using cached google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-24.3.7 gast-0.5.4 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.62.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b5f4dd-ae38-4706-a620-626652a38144",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.1-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.1-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e632cf7c-d75d-45c1-a371-4a7e4d6509c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\man\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Using cached regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Using cached safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "Using cached tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.21.4 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66ff2b4-0940-4ae9-8f07-6be1aad32d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\anaconda3\\envs\\man\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense,Input,Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21782e8-94ea-4089-8060-adf3dfab4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/public_dat/train.csv\")\n",
    "df['Sentence'] = df['Sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c97c58-adb7-4f8f-a053-9c1ba82de66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Entity_1</th>\n",
       "      <th>Entity_2</th>\n",
       "      <th>Relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rising interest rates affects borrowing costs ...</td>\n",
       "      <td>Interest Rates</td>\n",
       "      <td>borrowers</td>\n",
       "      <td>affects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>market volatility affects traders by leading t...</td>\n",
       "      <td>Market Volatility</td>\n",
       "      <td>Traders</td>\n",
       "      <td>affects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>strong corporate earnings affect shareholders ...</td>\n",
       "      <td>Corporate Earnings</td>\n",
       "      <td>Shareholders</td>\n",
       "      <td>affects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>regulatory changes affect financial institutio...</td>\n",
       "      <td>Regulatory Changes</td>\n",
       "      <td>Financial Institutions</td>\n",
       "      <td>affects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>investor behavior affects market trends and as...</td>\n",
       "      <td>Investor Behavior</td>\n",
       "      <td>Market Trends</td>\n",
       "      <td>affects</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                           Sentence            Entity_1  \\\n",
       "0   1  rising interest rates affects borrowing costs ...      Interest Rates   \n",
       "1   3  market volatility affects traders by leading t...   Market Volatility   \n",
       "2   4  strong corporate earnings affect shareholders ...  Corporate Earnings   \n",
       "3   5  regulatory changes affect financial institutio...  Regulatory Changes   \n",
       "4   6  investor behavior affects market trends and as...   Investor Behavior   \n",
       "\n",
       "                 Entity_2 Relation  \n",
       "0               borrowers  affects  \n",
       "1                 Traders  affects  \n",
       "2            Shareholders  affects  \n",
       "3  Financial Institutions  affects  \n",
       "4           Market Trends  affects  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebb15fb-b7fc-4f26-8602-13dac00cf855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rising interest rates affects borrowing costs ...\n",
       "1       market volatility affects traders by leading t...\n",
       "2       strong corporate earnings affect shareholders ...\n",
       "3       regulatory changes affect financial institutio...\n",
       "4       investor behavior affects market trends and as...\n",
       "                              ...                        \n",
       "7687    sharp, which sells aquos smartphones, fell zer...\n",
       "7688    short gross sales come when an investor sells ...\n",
       "7689    homeowners have a good chance of keeping their...\n",
       "7690    german broker lang & schwarz tradecenter ag, w...\n",
       "7691    global funds sold more south korean stocks tha...\n",
       "Name: Sentence, Length: 7692, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38680581-0c07-4f1f-974c-833c9c335936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences=[]\n",
    "for jok in df['Sentence']:\n",
    "    sentences.append(jok)\n",
    "word1=[]\n",
    "for jok in df['Entity_1']:\n",
    "    word1.append(jok)\n",
    "word2=[]\n",
    "for jok in df['Entity_2']:\n",
    "    word2.append(jok)\n",
    "relations=[]\n",
    "for jok in df['Relation']:\n",
    "    relations.append(jok)\n",
    "    \n",
    "# Separate data into input (sentences) and output (relations)\n",
    "#sentences = np.array(sentences)\n",
    "#word1 = np.array([df['Entity_1']])\n",
    "#word2 = np.array([df['Entity_2']])\n",
    "#relations = np.array([df['Relation']])\n",
    "#sentences=sentences.flatten()\n",
    "#word1=word1.flatten()\n",
    "#word2=word2.flatten()\n",
    "#relations=relations.flatten()\n",
    "#print(relations)\n",
    "# Tokenize sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d9ecad-a0e3-43f5-8c0b-f480b26eb82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be391e-a9de-4568-bc78-aed2da922a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences + word1 + word2)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Encode sentences and relations\n",
    "X_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "X_word1 = tokenizer.texts_to_sequences(word1)\n",
    "X_word2 = tokenizer.texts_to_sequences(word2)\n",
    "max_len = 256 #max(len(seq) for seq in X_sentences)\n",
    "X_sentences = pad_sequences(X_sentences, maxlen=max_len)\n",
    "X_word1 = pad_sequences(X_word1, maxlen=1)\n",
    "X_word2 = pad_sequences(X_word2, maxlen=1)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(relations)  # Assuming relations are categorical strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d5a1c05-552b-4f1c-942b-6fed3979f8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625ec7c9-9e1c-421b-a5d4-ad5e61a003c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c684700c-1c25-4893-b2ae-11ee64713b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3b1277-8933-4336-923c-759f0758a02a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\envs\\man\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m tokenized_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m tokenized_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 24\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(relations), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[0;32m     27\u001b[0m X_train, X_test, token_type_ids_train, token_type_ids_test, attention_masks_train, attention_masks_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     28\u001b[0m     input_ids, token_type_ids, attention_masks, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     29\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=12)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set maximum sequence length\n",
    "max_length = 128  # You can adjust this value as needed\n",
    "\n",
    "# Tokenize input sentences\n",
    "tokenized_inputs = tokenizer(sentences, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert tokenized inputs to tensors\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "token_type_ids = tokenized_inputs['token_type_ids']\n",
    "attention_masks = tokenized_inputs['attention_mask']\n",
    "labels = torch.tensor(label_encoder.fit_transform(relations), dtype=torch.long)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, token_type_ids_train, token_type_ids_test, attention_masks_train, attention_masks_test, y_train, y_test = train_test_split(\n",
    "    input_ids, token_type_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Move data and model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "X_train, X_test = X_train.to(device), X_test.to(device)\n",
    "token_type_ids_train, token_type_ids_test = token_type_ids_train.to(device), token_type_ids_test.to(device)\n",
    "attention_masks_train, attention_masks_test = attention_masks_train.to(device), attention_masks_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "\n",
    "# Define DataLoader for training and validation\n",
    "train_dataset = TensorDataset(X_train, token_type_ids_train, attention_masks_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, token_type_ids_test, attention_masks_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define optimizer and criterion\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, token_type_ids, attention_masks, labels = [data.to(device) for data in batch]\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
    "            pbar.set_postfix({'Loss': total_loss / (pbar.n + 1), 'Accuracy': total_accuracy / ((pbar.n + 1) * len(labels))})\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5495b9-479e-4d33-a980-5e552ee4806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define a label encoder\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dc94eac-64a8-4e10-9dfc-8750c36df0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: Train Loss: 1.9412, Train Acc: 0.3961, Val Loss: 0.6027, Val Acc: 0.8967\n",
      "Epoch 2/3: Train Loss: 0.4455, Train Acc: 0.9038, Val Loss: 0.2947, Val Acc: 0.9266\n",
      "Epoch 3/3: Train Loss: 0.2806, Train Acc: 0.9309, Val Loss: 0.2723, Val Acc: 0.9253\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=12)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set maximum sequence length\n",
    "max_length = 128  # You can adjust this value as needed\n",
    "\n",
    "# Tokenize input sentences\n",
    "tokenized_inputs = tokenizer(sentences, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert tokenized inputs to tensors\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "token_type_ids = tokenized_inputs['token_type_ids']\n",
    "attention_masks = tokenized_inputs['attention_mask']\n",
    "labels = torch.tensor(label_encoder.fit_transform(relations), dtype=torch.long)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, token_type_ids_train, token_type_ids_test, attention_masks_train, attention_masks_test, y_train, y_test = train_test_split(\n",
    "    input_ids, token_type_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Move data and model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "X_train, X_test = X_train.to(device), X_test.to(device)\n",
    "token_type_ids_train, token_type_ids_test = token_type_ids_train.to(device), token_type_ids_test.to(device)\n",
    "attention_masks_train, attention_masks_test = attention_masks_train.to(device), attention_masks_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "\n",
    "# Define DataLoader for training and validation\n",
    "train_dataset = TensorDataset(X_train, token_type_ids_train, attention_masks_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, token_type_ids_test, attention_masks_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define optimizer and criterion\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, token_type_ids, attention_masks, labels = [data.to(device) for data in batch]\n",
    "        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
    "    average_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = total_accuracy / len(train_dataset)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, token_type_ids, attention_masks, labels = [data.to(device) for data in batch]\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
    "    average_val_loss = total_loss / len(test_dataloader)\n",
    "    val_accuracy = total_accuracy / len(test_dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {average_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {average_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "    \n",
    "def predict(model, tokenizer, label_encoder, sentence2, device, max_length=128):\n",
    "    # Tokenize input sentence\n",
    "    inputs = tokenizer(sentence2, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_masks = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "    \n",
    "    # Get predicted label index\n",
    "    predicted_label_index = torch.argmax(outputs.logits).item()\n",
    "    \n",
    "    # Convert index to text label\n",
    "    predicted_label_text = label_encoder.inverse_transform([predicted_label_index])[0]\n",
    "    \n",
    "    return predicted_label_text\n",
    "\n",
    "# Example usage\n",
    "pd=[]\n",
    "for j in (sentences2):\n",
    "    sentence = j\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    predicted_label = predict(model, tokenizer, label_encoder, sentence, device)\n",
    "    pd.append(predicted_label)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5ed77d6-2142-45b6-8c96-b0bd80f5c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, label_encoder, sentence2, device, max_length=128):\n",
    "    # Tokenize input sentence\n",
    "    inputs = tokenizer(sentence2, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_masks = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks)\n",
    "    \n",
    "    # Get predicted label index\n",
    "    predicted_label_index = torch.argmax(outputs.logits).item()\n",
    "    \n",
    "    # Convert index to text label\n",
    "    predicted_label_text = label_encoder.inverse_transform([predicted_label_index])[0]\n",
    "    \n",
    "    return predicted_label_text\n",
    "\n",
    "# Example usage\n",
    "pd=[]\n",
    "for j in (sentences2):\n",
    "    sentence = j\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    predicted_label = predict(model, tokenizer, label_encoder, sentence, device)\n",
    "    pd.append(predicted_label)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237039e-c174-41ea-8769-5cc2170a15f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define LSTM model with named input layers\n",
    "embedding_dim = 150\n",
    "num_classes = len(set(relations))\n",
    "\n",
    "input_sentences = Input(shape=(max_len,), name='input_sentences')\n",
    "#input_word1 = Input(shape=(1,), name='input_word1')\n",
    "#input_word2 = Input(shape=(1,), name='input_word2')\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "lstm_layer = LSTM(units=256, dropout=0.0, recurrent_dropout=0.0)\n",
    "lstm_layer2 = LSTM(units=128, dropout=0.0, recurrent_dropout=0.0)\n",
    "lstm_layer3 = LSTM(units=64, dropout=0.0, recurrent_dropout=0.0)\n",
    "lstm_layer4 = LSTM(units=32, dropout=0.0, recurrent_dropout=0.0)\n",
    "dense_layer = Dense(num_classes, activation='softmax')\n",
    "\n",
    "embedding_sentences = embedding_layer(input_sentences)\n",
    "#embedding_word1 = embedding_layer(input_word1)\n",
    "#embedding_word2 = embedding_layer(input_word2)\n",
    "\n",
    "lstm_output_sentences = lstm_layer(embedding_sentences)\n",
    "embedding_layer2 = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "embedding_sentences2 = embedding_layer2(lstm_output_sentences)\n",
    "lstm_output_word1 = lstm_layer2(embedding_sentences2)\n",
    "embedding_layer3 = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "embedding_sentences3 = embedding_layer3(lstm_output_word1)\n",
    "lstm_output_word2 = lstm_layer3(embedding_sentences3)\n",
    "embedding_layer4 = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "embedding_sentences4 = embedding_layer4(lstm_output_word2)\n",
    "lstm_output_word3 = lstm_layer4(embedding_sentences4)\n",
    "#lstm_output_word12 = lstm_layer(embedding_word1)\n",
    "#lstm_output_word22 = lstm_layer(embedding_word2)\n",
    "\n",
    "concatenated = Concatenate()([lstm_output_sentences,lstm_output_word1,lstm_output_word2,lstm_output_word3])\n",
    "output = dense_layer(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e6140-ebfb-447e-8188-f6a8d0dfa077",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(\"Shape of X_sentences_train:\", X_sentences_train.shape)\n",
    "#print(\"Shape of X_sentences_test:\", X_sentences_test.shape)\n",
    "#print(X_sentences_train)\n",
    "#max_len\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c776b-3538-4f17-a24b-766ba4397898",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model(inputs=[input_sentences], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "learning_rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-7\n",
    "decay = 0.0\n",
    "optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, decay=decay)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit({\"input_sentences\": X_sentences_train}, y_train, epochs=10, batch_size=20, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate({\"input_sentences\": X_sentences_test}, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04621a-668b-413f-aa21-df2ce133f0e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate({\"input_sentences\": X_sentences_test}, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e64e5574-f22e-49c6-951d-94f1c1d55a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"D:/public_dat/test.csv\")\n",
    "df2['Sentence'] = df2['Sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a0ec2ae-69a7-40a6-88ff-60f688664fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2=[]\n",
    "for jok in df2['Sentence']:\n",
    "    sentences2.append(jok)\n",
    "word12=[]\n",
    "for jok in df2['Entity_1']:\n",
    "    word12.append(jok)\n",
    "word22=[]\n",
    "for jok in df2['Entity_2']:\n",
    "    word22.append(jok)\n",
    "    \n",
    "# Separate data into input (sentences) and output (relations)\n",
    "#sentences = np.array(sentences)\n",
    "#word1 = np.array([df['Entity_1']])\n",
    "#word2 = np.array([df['Entity_2']])\n",
    "#relations = np.array([df['Relation']])\n",
    "#sentences=sentences.flatten()\n",
    "#word1=word1.flatten()\n",
    "#word2=word2.flatten()\n",
    "#relations=relations.flatten()\n",
    "#print(relations)\n",
    "# Tokenize sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84da715e-234e-417f-aa3b-9732cef54adc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       8920\n",
      "1         64\n",
      "2       2755\n",
      "3       6871\n",
      "4       7305\n",
      "        ... \n",
      "1315    8344\n",
      "1316    5943\n",
      "1317    4424\n",
      "1318    4457\n",
      "1319    6012\n",
      "Name: Id, Length: 1320, dtype: int64 ['determines', 'affects', 'affects', 'affects', 'involves', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'manages', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'determines', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'affects', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'offers', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'invests', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'impacts', 'creates', 'creates', 'affects', 'creates', 'creates', 'creates', 'creates', 'invests', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'generates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'creates', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'impacts', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'affects', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'represents', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'generates', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'determines', 'involves', 'determines', 'determines', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'invests', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'invests', 'generates', 'generates', 'offers', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'provides', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'manages', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'invests', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'offers', 'invests', 'generates', 'generates', 'represents', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'generates', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'affects', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'represents', 'invests', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'creates', 'impacts', 'impacts', 'impacts', 'creates', 'impacts', 'impacts', 'impacts', 'impacts', 'creates', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'involves', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'creates', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'impacts', 'invests', 'impacts', 'impacts', 'impacts', 'impacts', 'sells', 'impacts', 'invests', 'invests', 'invests', 'represents', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'sells', 'invests', 'invests', 'invests', 'determines', 'invests', 'invests', 'invests', 'invests', 'invests', 'creates', 'represents', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'manages', 'represents', 'invests', 'invests', 'sells', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'sells', 'invests', 'invests', 'sells', 'invests', 'invests', 'invests', 'invests', 'invests', 'sells', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'creates', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'provides', 'invests', 'invests', 'invests', 'invests', 'invests', 'invests', 'manages', 'invests', 'invests', 'invests', 'involves', 'invests', 'invests', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'determines', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'sells', 'involves', 'determines', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'sells', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'provides', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'represents', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'provides', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'represents', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'determines', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'involves', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'determines', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'sells', 'manages', 'offers', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'sells', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'affects', 'invests', 'manages', 'manages', 'manages', 'invests', 'invests', 'manages', 'manages', 'manages', 'manages', 'represents', 'manages', 'creates', 'manages', 'manages', 'manages', 'manages', 'sells', 'manages', 'manages', 'manages', 'manages', 'manages', 'offers', 'manages', 'involves', 'manages', 'manages', 'manages', 'manages', 'provides', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'invests', 'manages', 'manages', 'manages', 'invests', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'manages', 'offers', 'provides', 'offers', 'represents', 'offers', 'offers', 'offers', 'offers', 'provides', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'provides', 'offers', 'offers', 'provides', 'provides', 'offers', 'offers', 'determines', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'provides', 'offers', 'offers', 'offers', 'provides', 'provides', 'offers', 'invests', 'provides', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'invests', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'provides', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'offers', 'provides', 'manages', 'provides', 'provides', 'sells', 'provides', 'provides', 'provides', 'offers', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'offers', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'creates', 'provides', 'provides', 'provides', 'provides', 'represents', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'offers', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'determines', 'provides', 'provides', 'provides', 'provides', 'provides', 'sells', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'invests', 'provides', 'provides', 'involves', 'provides', 'provides', 'represents', 'provides', 'provides', 'provides', 'provides', 'provides', 'provides', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'manages', 'provides', 'represents', 'represents', 'represents', 'represents', 'manages', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'generates', 'represents', 'represents', 'represents', 'represents', 'represents', 'determines', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'manages', 'represents', 'provides', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'represents', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'offers', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'provides', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'invests', 'sells', 'sells', 'sells', 'sells', 'sells', 'manages', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'provides', 'sells', 'sells', 'sells', 'sells', 'sells', 'provides', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'affects', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'sells', 'offers', 'sells', 'sells', 'sells']\n"
     ]
    }
   ],
   "source": [
    "print(df2['Id'],pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce5f19db-184d-4988-bc3c-d1f61177c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4393989-8f69-4a0b-84bc-34399b91b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for predicted_label in zip(pd):\n",
    "    #print(f\"Predicted Label: {predicted_label}\")\n",
    "    #print()\n",
    "import pandas as pd\n",
    "data={'Id':df2['Id'],'Relation':pdf}\n",
    "dfk=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c1015ce-3e18-4b6a-9565-eb4808a9437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id    Relation\n",
      "0     8920  determines\n",
      "1       64     affects\n",
      "2     2755     affects\n",
      "3     6871     affects\n",
      "4     7305    involves\n",
      "...    ...         ...\n",
      "1315  8344       sells\n",
      "1316  5943      offers\n",
      "1317  4424       sells\n",
      "1318  4457       sells\n",
      "1319  6012       sells\n",
      "\n",
      "[1320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f12e1512-72f2-42a3-a9f1-f01abe0181bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# saving the dataframe\n",
    "dfk.to_csv(r'D:\\Lithin\\codlang\\Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688b4f1-0a3c-4885-bf35-1980df2cd2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfbba3-3f46-4ff6-bf00-1114c9c86b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aef006-e1d6-4e39-a4b2-8f5c3f4c707c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9eb9d0-9dfa-40cb-a944-f2ac20f67d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c7017-a917-410c-942f-93257caa01af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbf41f-9bc0-475e-a3f3-bd2219f5cbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1220f0-fe94-4109-9a63-ad31046d83c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3b803-6ffb-4e25-ba29-88b526c33daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a74f94-a242-4522-87cb-6bbb601cf786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f048c-6a81-4f5f-8b25-0e0e29d00575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e92447-3193-426f-b5c4-1efb63f6a87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb605193-7484-48b5-9f22-fb67383566a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e395eb-a01d-48df-a478-164b5e1f6955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68310a5-d786-4dd8-9562-8af54712555d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fea29-5654-4982-83b8-77f8d134e871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586cf12-0785-4d0d-93c7-8f0274ef713f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294f8d4-99ac-4101-b5c4-7427c688b99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4b614-2e36-45b9-88e0-254b731268c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675699e7-8c53-49bc-a968-dc731d7fa1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "man"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
